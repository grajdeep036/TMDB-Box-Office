{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\nsample_submission = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows(train): \"+str(len(train)))\nprint(\"Number of rows(test): \"+str(len(test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_collection'] = train['belongs_to_collection'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntrain['collection_id'] = train['belongs_to_collection'].apply(lambda x: eval(x)[0]['id'] if str(x) != 'nan' else 0)\n\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntest['collection_id'] = test['belongs_to_collection'].apply(lambda x: eval(x)[0]['id'] if str(x) != 'nan' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else []).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter([i for j in list_of_genres for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\n\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if (isinstance(x,int) or isinstance(x,str)) == True else '')\nfor gen in top_genres:\n    train['genre_' + gen] = train['all_genres'].apply(lambda x: 1 if gen in x else 0)\n    \ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if (isinstance(x,int) or isinstance(x,str)) == True else '')\nfor gen in top_genres:\n    test['genre_' + gen] = test['all_genres'].apply(lambda x: 1 if gen in x else 0)\n    \ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_encoder = LabelEncoder()\ntrain['all_genres'] = lang_encoder.fit_transform(train['all_genres'])\ntest['all_genres'] = lang_encoder.fit_transform(test['all_genres'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['original_language'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_encoder = LabelEncoder()\ntrain['original_language'] = lang_encoder.fit_transform(train['original_language'])\ntest['original_language'] = lang_encoder.fit_transform(test['original_language'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"prod_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else '').values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prod_companies_count'] = train['production_companies'].apply(lambda x: len([i for i in eval(x)]) if str(x) != 'nan' else 0)\ntest['prod_companies_count'] = test['production_companies'].apply(lambda x: len([i for i in eval(x)]) if str(x) != 'nan' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to make a new feature called \"production score\". Score will be calculated on the popularity of production company( that is given by the number of times it has appeared in the dataset). If the movie have multiple productions then the score will be assigned by the most popular production."},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_production = Counter([i for j in prod_companies for i in j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['production_score'] = train['production_companies'].apply(lambda x: np.tanh(max([pop_production[i['name']] for i in eval(x)])) if str(x) != 'nan' else 0)\ntest['production_score'] = test['production_companies'].apply(lambda x: np.tanh(max([pop_production[i['name']] for i in eval(x)])) if str(x) != 'nan' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['production_companies'], axis=1)\ntest = test.drop(['production_companies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['production_countries'] = train['production_countries'].apply(lambda x: [i['name'] for i in eval(x)][0] if str(x) != 'nan' else '')\ntest['production_countries'] = test['production_countries'].apply(lambda x: [i['name'] for i in eval(x)][0] if str(x) != 'nan' else '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prod_country_encoder = LabelEncoder()\ntrain['production_countries'] = prod_country_encoder.fit_transform(train['production_countries'])\ntest['production_countries'] = prod_country_encoder.fit_transform(test['production_countries'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['production_countries'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Release Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['release_date'] = train['release_date'].apply(lambda x: pd.to_datetime(x))\ntest['release_date'] = test['release_date'].apply(lambda x: pd.to_datetime(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = train['release_date'].apply(lambda x: x.year)\ntrain['month'] = train['release_date'].apply(lambda x: x.month)\ntrain['day_of_week'] = train['release_date'].apply(lambda x: x.weekday())\n\ntest['year'] = test['release_date'].apply(lambda x: x.year)\ntest['month'] = test['release_date'].apply(lambda x: x.month)\ntest['day_of_week'] = test['release_date'].apply(lambda x: x.weekday())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['release_date'], axis=1)\ntest = test.drop(['release_date'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in release date"},{"metadata":{},"cell_type":"markdown","source":"From the plot below we can observe a very weird trend that there are a lot of movies that have a release date like 2067, 2024 and so on. It turns out that this is just a typo. For example, the movie *Major Dundee* was released in 1965 whereas in the dataset it's release year is 2065. So we can just replace it with 1965.\n\n![Major Dundee](https://image.tmdb.org/t/p/w600_and_h900_bestv2/skv6Jsw6YyPKV4oQhs88zvwDCAL.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.lineplot(x=train['year'], y=train['revenue'], color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = train['year'].apply(lambda x: x-100 if x>2020 else x)\ntest['year'] = test['year'].apply(lambda x: x-100 if x>2020 else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.lineplot(x=train['year'], y=train['revenue'], color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Runtime"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_runtime_train = train['runtime'].mean()\ntrain['runtime'] = train['runtime'].apply(lambda x: x if str(x) != 'nan' else avg_runtime_train)\n\navg_runtime_test = test['runtime'].mean()\ntest['runtime'] = test['runtime'].apply(lambda x: x if str(x) != 'nan' else avg_runtime_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.distplot(train['runtime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Budget"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['budget']==0].head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spoken Languages"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['spoken_languages_count'] = train['spoken_languages'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntest['spoken_languages_count'] = test['spoken_languages'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.countplot(x=train['spoken_languages_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['spoken_languages'], axis=1)\ntest = test.drop(['spoken_languages'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Keywords']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else []).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_Keywords'] = train['Keywords'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if str(x) != 'nan' else '')\n\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\n\nfor g in top_keywords:\n    train['keyword_'+g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \n    \n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if str(x) != 'nan' else '')\n\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\n\nfor g in top_keywords:\n    test['keyword_'+g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_encoder = LabelEncoder()\ntrain['all_Keywords'] = keywords_encoder.fit_transform(train['all_Keywords'])\ntest['all_Keywords'] = keywords_encoder.fit_transform(test['all_Keywords'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['Keywords'], axis=1)\ntest = test.drop(['Keywords'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cast"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cast'] = train['cast'].apply(lambda x: eval(x) if str(x) != 'nan' else [])\ntest['cast'] = test['cast'].apply(lambda x: eval(x) if str(x) != 'nan' else [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['gender_0_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['gender_1_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['gender_2_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\ntest['gender_0_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['gender_1_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['gender_2_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_members = list(train['cast'].apply(lambda x: [i['name'] for i in x] if str(x) != 'nan' else []).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_cast_members = [m[0] for m in Counter([i for j in list_of_cast_members for i in j]).most_common(50)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for g in top_cast_members:\n    train['cast_member_'+g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \nfor g in top_cast_members:\n    test['cast_member_'+g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['crew'] = train['crew'].apply(lambda x: eval(x) if str(x) != 'nan' else [])\ntest['crew'] = test['crew'].apply(lambda x: eval(x) if str(x) != 'nan' else [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_members = list(train['crew'].apply(lambda x: [i['name'] for i in x] if str(x) != 'nan' else []).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_crew_members = [m[0] for m in Counter([i for j in list_of_crew_members for i in j]).most_common(50)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for g in top_crew_members:\n    train['crew_member_'+g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\n    \nfor g in top_crew_members:\n    test['crew_member_'+g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Homepage"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_homepage'] = train['homepage'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntest['has_homepage'] = test['homepage'].apply(lambda x: 1 if str(x) != 'nan' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['homepage'], axis=1)\ntest = test.drop(['homepage'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Poster Path"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['poster_path'], axis=1)\ntest = test.drop(['poster_path'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['status'], axis=1)\ntest = test.drop(['status'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Title, Tagline, Overview, Original Title"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    \n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop([\"imdb_id\", \"original_title\", \"overview\", \"tagline\", \"title\"], axis=1)\ntest = test.drop([\"imdb_id\", \"original_title\", \"overview\", \"tagline\", \"title\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'revenue'], axis=1)\nY = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel.fit(X_train, Y_train, \n        eval_set=[(X_train, Y_train), (X_valid, Y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_valid = model.predict(X_valid)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['revenue'] = np.expm1(y_pred)\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}